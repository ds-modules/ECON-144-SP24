{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c706b8e-48e9-4ff3-a3cd-ce4b257bf32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install otter-grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2ea020-3fba-4963-818a-879a0f6befb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "733c52b8-e59e-4234-9c8e-e9b5f2a5e882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "# If you need to install Otter, please uncomment and run the previous cell\n",
    "import otter\n",
    "grader = otter.Notebook(\"ps3.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4b578d-68fc-4216-a155-229446d2e07d",
   "metadata": {},
   "source": [
    "# Econ 144 â€“ Problem Set 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee37bb4c-6073-4d54-be7e-59693ee056bb",
   "metadata": {},
   "source": [
    "In this problem set, there will be much less guidance on how to conduct any given analysis. You can think of it as a mini-project, where you make use of many of the tools you have seen over the course of the first two problem sets.\n",
    "\n",
    "Throughout the entire problem set, please feel free to add code and markdown cells as you need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d80d23-b435-49c7-98f6-f5148fbab980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.tsa.api as smt\n",
    "from scipy.stats import skew \n",
    "from scipy.stats import kurtosis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4821cda2-4212-4cdd-bc72-97aad22a508e",
   "metadata": {},
   "source": [
    "## Problem 1. Exploratory Data Analysis\n",
    "\n",
    "Using daily and monthly returns data for ten individual stocks and the equal-weighted and value-weighted CRSP market indexes (`ewretd` and `vwretd`) -- from the provided excel files -- perform the outlined statistical analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f711e3a-a405-4c62-9442-f947345abd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "stkdata = pd.read_excel('daily_stock1988-2022.xlsx', index_col=0)\n",
    "stkdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1670273-0508-41fc-ab8f-59bfff8e7342",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxdata = pd.read_excel('daily_index1988-2022.xlsx', index_col=0)\n",
    "idxdata.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f5a810-d40f-4420-82e5-594b1ea3d059",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question 1.a.**\n",
    "Compute the sample mean $\\hat{\\mu}$, sample standard deviation $\\hat{\\sigma}$, and the autocorrelation coefficient at 1 lag $\\hat{\\tau}_1$ for daily simple returns over the entire sample period (1988-2022) for the ten stocks and two indexes. Split the sample into three subperiods (1988-1999, 2000-2011, 2012-2022) and compute the same statistics in each subperiod. Are the statistics stable over the subperiods?\n",
    "\n",
    "\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_a\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbae6bf-713e-4329-9b5b-f92d1654cb56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efcfc4d6-adfc-4163-825d-c857a790baf6",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question 1.b.**\n",
    "Compute the sample mean $\\hat{\\mu}$, sample standard deviation $\\hat{\\sigma}$, and the autocorrelation coefficient at 1 lag $\\hat{\\tau}_1$ for continuously compounded daily returns over the entire sample period (1988-2022) for the ten stocks and two indexes. Split the sample into three subperiods (1988-1999, 2000-2011, 2012-2022) and compute the same statistics in each subperiod. Can continuous compounding substantially change inferences?\n",
    "\n",
    "\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_b\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366328de-59ba-49fd-a1b6-c0ace6b41366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95739a0f-5bdb-486a-bc5c-438c8467eb03",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question 1.c.**\n",
    "Plot histograms of daily simple returns for `ewretd` and `vwretd` over the entire sample period. Plot another histogram of the normal distribution with mean and standard deviation equal to the sample mean and sample standard deviation of the returns plotted in the first histograms. Do daily simple returns look approximately normal? Whis looks closeer to normal: `ewretd` or `vwretd`? \n",
    "\n",
    "Perform the same analysis for continuously compounded daily returns and compare the results to those for the daily simple returns.\n",
    "\n",
    "\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_c\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702f5556-11c0-4208-bf19-d9044501423c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06b2d718-be22-4c51-9ee2-f1da4de32ee5",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question 1.d.**\n",
    "Using daily simple returns for the entire sample period, construct 99% confidence intervals for $\\hat{\\mu}$ for `ewretd`, `vwretd`, and the ten individual stock series.\n",
    "\n",
    "Do the same for each of the subperiods. Do the confidence intervals shift  agreat deal over time (i.e., over the three subperiods)?\n",
    "\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_d\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0eca20-e42f-46a0-a164-05b78cf78d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f5457fa-de74-4dfd-921b-bda330f26721",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question 1.e.**\n",
    "Compute the skewness and kurtosis of daily simple returns for `ewretd`, `vwretd`, and the ten individual stock series over the entire sample period, and in each of the three subperiods. Are any of the skewness and kurtosis values statistically different from the skewness and kurtosis of a normal raanom variable (at the 5% level)?\n",
    "\n",
    "For `ewretd`, `vwretd`, and the ten individual stock series, perform the same calculations using the monthly data. \n",
    "\n",
    "What do you conclude about the normality of these return series? Explain.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_e\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7318c7aa-a794-457c-a21b-17766b04d9a6",
   "metadata": {},
   "source": [
    "## Problem 2. Testing Predictability\n",
    "\n",
    "In this problem, you will analyze the predictability of stock returns with respect to the random walk (RW) hypothesis. You will look at this question using both stock index data and stock portfolio data.\n",
    "\n",
    "The stock index data for this exercise is in the file `daily_index1988-2023.xslx`. The overall period covered by the data is January 1988 through December 2023. You will examine the entire period, as well as three 12-year sub-periods: January 1988 - December 1999, January 2000 - December 2011, and January 2012 - December 2023. The analysis focuses on the CRSP value-weighted index return `vwretd` and the CRSP equal-weighted index return `ewretd`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d831aa2-7af7-4ecd-96cc-3256128b2fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxdata = pd.read_excel('daily_index1988-2023.xlsx', index_col=0)\n",
    "idxdata['sub1'] = np.int64((idxdata.index >= 19880101) & (idxdata.index <= 19991231))\n",
    "idxdata['sub2'] = np.int64((idxdata.index >= 20000101) & (idxdata.index <= 20111231))\n",
    "idxdata['sub3'] = np.int64((idxdata.index >= 20120101) & (idxdata.index <= 20231231))\n",
    "idxdata.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d727a0-518c-409a-a5a5-0cf02049e547",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question 2.a.**\n",
    "Plot correlograms for the daily CRSP value-weighted returns for our four periods; 198801-202312, 198801-199912, 200001-201112, and 201201-202312. Please see the document `return_predict.pdf` (on bCourses, under Files > Class Notes) to see what your output should look like.\n",
    "\n",
    "Please briefly comment on the correlograms with respect to the statistical significance and pattern of the autocorrelatios, both within and across the different periods.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_a\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4d5489-2738-45e2-ab53-b48048d408ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9fba92d-5f40-4f16-8939-d7a01d3ee1c0",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question 2.b.**\n",
    "Plot correlograms for the daily CRSP equal-weighted returns for our four periods; 198801-202312, 198801-199912, 200001-201112, and 201201-202312. Please see the document `return_predict.pdf` (on bCourses, under Files > Class Notes) to see what your output should look like.\n",
    "\n",
    "Please briefly comment on the correlograms with respect to the statistical significance and pattern of the autocorrelatios, both within and across the different periods. Compare to the correlograms for the value-weighted returns.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_b\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c0525b-6319-4f2c-ab96-d747066a2eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "140a02c7-591c-4b5b-8d7d-ee9901b358fc",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question 2.c.**\n",
    "Reproduce the results in the table from the page titled \"Autocorrelation in daily stock index returns, 1988-2023\" (`return_predict.pdf`). You do not need to reproduce the table as presented on the slide, but you should display the numerical results in some readable fashion. In your submission, you must include the code you used to do this.\n",
    "\n",
    "Interpret the results with respect to the random walk hypothesis, statistical significance, and economic significance.\n",
    "\n",
    "In empirical research, one of the first tasks we often undertake is to try and reproduce results presented in a published paper (or publicly available working paper). This can give you insight into the analysis methods employed by the authors, as well as a jumping off point for verifying assumptions and results. \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_c\n",
    "manual: true\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22731a5-a55c-43de-a952-8839c0a70392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fdc2716-eaa1-4f5d-b808-8ecdc23a6334",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question 2.d.**\n",
    "Reproduce the results in the table from the page titled \"Variance ratios for daily stock index returns, 1988-2023\" (`return_predict.pdf`). You do not need to reproduce the table as presented on the slide, but you should display the numerical results in some readable fashion. In your submission, you must include the code you used to do this.\n",
    "\n",
    "Interpret the results with respect to the random walk hypothesis, statistical significance, and economic significance.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_c\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a962764c-2224-44b2-a020-869235f4e97f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a977f97b-6f48-4991-8f99-a360036ed740",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question 2.e.**\n",
    "The stock return data is in the file `stockdata_djia.xlsx`. The stock return data includes daily reutns for 19 stocks from the Dow Jones Industrial Average (DJIA). These stocks were chosen because they all have a full complement of daily returns over the period January 1988 through Decenber 2023. Hence there are 9,070 return observations for each of these 19 stocks.\n",
    "\n",
    "On each day in the period, each stock is assigned a portfolio number (`PORTNUM`) based on market capitalization. Portfolio 1 contains the 6 largest stocks (based on market capitalization at the beginning of each month), portfolio 2 contains the next 6 largest stocks, and portfolio 3 contains the 7 smallest stocks. Daily portfolio returns for each portfolio will be calculated as an **equal-weighted** return within the portfolio.\n",
    "\n",
    "**Note**: with this construction, individual stocks can be grouped into different portfolios over the course of the entire sample period.\n",
    "\n",
    "Based on the \"raw\" data, construct a dataframe that looks like the following, where the portfolio returns are the equal-weighted returns of the stocks in the portfolio, and `sub1`, `sub2`, and `sub3` refer to the three sub-periods (1) 198801-199912, (2) 200001-201112, and (3) 201201-202312.\n",
    "\n",
    "<img src=\"tableinfo.png\" width=\"400\"/>\n",
    "<img src=\"tablehead.png\" width=\"400\"/>\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_e\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47b202a-66ec-48b0-90c5-7cc316ce39c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stkdata = pd.read_excel('stockdata_djia.xlsx')\n",
    "stkdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde36b24-a005-42f2-9d46-4cd4eee50886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abbec9e3-7f87-4715-a535-58ff894a661a",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question 2.f.**\n",
    "Plot correlograms for the daily portfolio for our four periods; 198801-202312, 198801-199912, 200001-201112, and 201201-202312. Please see the document `return_predict.pdf` (on bCourses, under Files > Class Notes) to see what your output should look like.\n",
    "\n",
    "Please briefly comment on the correlograms with respect to the statistical significance and pattern of the autocorrelatios, both within and across the different periods. Compare the correlograms across the three portfolios.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_f\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d76ed1a-f45b-497c-9ba5-8e1accc512d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ed0e3a6-ab7d-4825-9442-43bdbcf1b00c",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question 2.g.**\n",
    "Reproduce the results in the table from the page titled \"Autocorrelation in daily portfolio returns, 1988-2023\" (`return_predict.pdf`). You do not need to reproduce the table as presented on the slide, but you should display the numerical results in some readable fashion. In your submission, you must include the code you used to do this.\n",
    "\n",
    "Interpret the results with respect to the random walk hypothesis, statistical significance, and economic significance.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_g\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10cb898-a346-48a1-9d9d-32671655c31f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdcb9e47-c9b6-49b5-b176-b31af5e98f6e",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question 2.h.**\n",
    "Reproduce the results in the table from the page titled \"Variance ratios for daily portfolio returns, 1988-2023\" (`return_predict.pdf`). You do not need to reproduce the table as presented on the slide, but you should display the numerical results in some readable fashion. In your submission, you must include the code you used to do this.\n",
    "\n",
    "Interpret the results with respect to the random walk hypothesis, statistical significance, and economic significance.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_h\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75137e09-dac9-4c2c-8781-75334a14aeb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "042dd1df-21aa-44de-8ab0-58f5519f3d11",
   "metadata": {},
   "source": [
    "## Problem 3. Regression Discontinuity\n",
    "\n",
    "The data set `rd.csv` contains student level data for 65,535 students who finished high\n",
    "school and were eligible to enter college. In the specific country where the data orginate\n",
    "(Chile), students write a standardized test at the end of high school, called the PSU test.\n",
    "Their scores on this test, plus high school GPA, determine which colleges they can get into.\n",
    "Students who score at least 475 points on the PSU test are also eligible for a loan from the\n",
    "government for college costs, while students who score less than 475 points cannot receive\n",
    "the loan. In this exercise we will use regression discontinuity methods to analyze the effect\n",
    "of the loan program on the probability of college entry.\n",
    "\n",
    "| Variable | Description |\n",
    "|-|-|\n",
    "| **psu** | PSU test score (ranges from 300 to 700) |\n",
    "| **over475** | 1=PSU score is 475 or higher |\n",
    "| **entercollege** | 1=student entered college |\n",
    "| **hsgpa** | high school GPA (ranges from 0 to 70) |\n",
    "| **privatehs** | 1=student went to privatre high school |\n",
    "| **hidad** | 1=father has more than a high school education |\n",
    "| **himom** | 1=mother has more than a high school education |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e86b222-7b08-4d64-89ba-4f91ab868763",
   "metadata": {},
   "outputs": [],
   "source": [
    "rd = pd.read_csv(\"rd.csv\")\n",
    "rd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840e2e0c-5480-44b8-be7d-13cb4de20c69",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question 3.a.**\n",
    "Construct the average values of `entercollege`, `hsgpa`, `privatehs`, `hidad`, `himom` for each integer value of `psu` (e.g., get the averages for scores from 300 to 300.99, and assign them to the \"300\" bucket; then get the averages for scores from 301 to 301.99 and assign them to the \"301\" bucket, etc.). This is sometimes called \"collapsing\" the data\n",
    "to integer cells. This is a bit tricky, so we provide the commands for you below (i.e., just run the code as written).\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3_a\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905b7e95-2263-4fed-91ed-269d1d8bda65",
   "metadata": {},
   "outputs": [],
   "source": [
    "rd['psu_integer'] = np.floor(rd['psu'])\n",
    "rd_temp = rd.groupby('psu_integer').agg(['mean']).reset_index()\n",
    "\n",
    "rd_collapsed = pd.DataFrame()\n",
    "rd_collapsed['psu_integer'] = rd_temp['psu_integer']\n",
    "rd_collapsed['hsgpa'] = rd_temp['hsgpa']['mean']\n",
    "rd_collapsed['psu'] = rd_temp['psu']['mean']\n",
    "rd_collapsed['entercollege'] = rd_temp['entercollege']['mean']\n",
    "rd_collapsed['privatehs'] = rd_temp['privatehs']['mean']\n",
    "rd_collapsed['hidad'] = rd_temp['hidad']['mean']\n",
    "rd_collapsed['himom'] = rd_temp['himom']['mean']\n",
    "rd_collapsed['over475'] = rd_temp['over475']['mean']\n",
    "rd_collapsed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581170f1-956e-4862-873c-9fce1cfd1556",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question 3.b.**\n",
    "Generate plots of the average values of `entercollege`, `hsgpa`, `privatehs`, `hidad`, `himom` (from 3.a) as a function of `psu` (be sure to label your axes and give each plot a title). You should see a jump in `entercollege` at 475 points, but relatively smooth values of the other variables. \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3_b\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d9aa68-5349-47a0-97f8-7c51fa93c4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603dad53-8c51-4d99-8113-e4846d2ffefd",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question 3.c.**\n",
    "Next you will fit *local linear* regressions using different bandwidths. To do this you will regress one of the dependent variables $Y_i$ on the following independent variables: `constant`, `psu`,`over475` and $\\tilde{psu} = psu - 475$, i.e., you will fit the model\n",
    "\n",
    "$Y_i = \\beta_0 + \\beta_1 over475_i + \\beta_2 \\tilde{psu}_i + \\delta_3 (\\tilde{psu}_i \\cdot over475_i) + u_i$\n",
    "\n",
    "Interpret the coefficients of this regression model.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3_c\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e871cd47-27d2-43b2-b8d8-dbdbe969b2b6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f112824-551c-4381-aeea-4479be87d862",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question 3.d.**\n",
    "Using the \"collapsed\" data from part 3.a, which has one observation per integer value of `psu_integer`, and a bandwidth of 10 on each side of the 475 cutoff, fit the model for each of the dependent variables $Y_i = entercollege$, $Y_i = hsgpa$,, $Y_i = hidad$, $Y_i = himom$ (i.e., you are fitting four separate models here). The following cell is for your code.\n",
    "\n",
    "*Hint: This means that you fit the regression models to the collapsed data for the subset of data with $465 \\leq psu\\_integer \\leq 485$. This data set will have 21 observations -- 10 observations for scores less than 475 and 11 observations for scores of 475 or higher.*\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3_d\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1638014-dc3c-488e-8927-18e8e3aa7530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e0b9407-9896-427a-ac14-d83380ce4c9a",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question 3.e.**\n",
    "Repeat part 3.d using a bandwidth of 20 points. Do you find that the estimated jumps are similar for all four dependent variables as with a bandwidth of 10?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3_e\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2838b86e-0753-4e9b-82c6-ad6c91fb02e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5a2c981-0f4e-4b2e-a12f-31faa7e98436",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3ccbac3-18cc-4b23-8c81-ca1a9105bb3f",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question 3.f.**\n",
    "For every bandwidth from 5 to 50, develop a plot to show the estimate of $\\beta_1$ when the dependent variable $Y_i$ is $entercollege$. \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3_f\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34595413-d1ce-4177-a089-6e8ce6863227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "035dc76a-3e23-42c7-b4ff-a5f6cca6577e",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a pdf file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4333b93f-c91c-4dc2-bff1-0cd860b324ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <p>Your file has been exported. Download it by right-clicking\n",
       "            <a href=\"ps3.pdf\" target=\"_blank\">here</a> and selecting <strong>Save Link As</strong>.\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.to_pdf(pagebreaks=False, display_link=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461ff688-5787-43b6-be09-6b938411d50b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
